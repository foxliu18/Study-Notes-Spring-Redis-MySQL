<center><b><h1>分布式系统面试</h1></b></center>

# 简述CAP理论

- 数据一致性(consistency): 如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者来演数据具有强一致性(strong consistency)
- 服务可用性(availability): 所有读写请求应在一定时间内得到响应，可终止，不会一直等待
- 分区容错性(partition-tolerance): 在网络分区的情况下，被分隔的节点仍能正常对外服务

单体系统保证CA，

分布式系统只能保证AP或CP，分布式系统不能放弃P，只能二选一

# 简述base理论

CAP理论的一种妥协，由于CAP只能二选其一，base理论**降低了发生分区容错是对可用性和一致性的要求**

1. 基本可用: 允许可用性降低（可能响应延长，可能服务降级）
2. 软状态: 指允许系统中的数据存在中间状态，并认为该中间状态不会影响系统整体可用性
3. 最终一致性: 节点数据同步可以存在时间延长，但在一定的期限后必须达成数据的一致，状态变为最终状态

# 数据一致性模型

强一致性：当更新操作完成后，任何多个后续进程的访问都会分会最新更新过的值，这种是对用户最友好的，就是用户上一次写什么，下一次就能保证读到什么。根据CAP理论，这种实现需要牺牲可用性。

弱一致性：系统在写入成功后，不承诺立即可以读到最新更新的值，也不会具体的承诺多久之后可以读到。用户读到某一操作对系统数据的更新需要一段时间，我们称这段时间为"不一致性窗口"

最终一致性: 最终一致性是弱一致性的特例, 强调的是所有数据副本,在经过一段时间的同步后,最终能达到一个一致的状态. 因此,最终一致性的本事是需要系统保证最终数据能够达到一直,而不需要实时保证系统数据的强一致性.达到最终一致性的时间,就是不一致窗口时间,在没有故障发生的前提下,不一致窗口的时间主要受通信延迟,系统负载和复制副本的个数影响.

最终一致性模型根据其提供的不同保证可以划分为更多的模型,包括因果一致性和会话一致性

# Quorum、WARO机制

waro(write all read one): 一种简单的副本控制协议,写操作时,只有当所有的副本都更新成功后,这次写操作才算成功,否则视为失败.优先保证读,任何节点读到的数据都是最新数据,牺牲了跟新服务的可用性,只要有一个副本宕机了,写服务就不会成功.但只要一个节点存活,就仍然能提供度服务

Quorum机制:10个副本,一次成功的更新了三个,那么最少要读取八个副本数据,可以保证读取到了最新的数据.无法保证强一致性,也就是无法实现任何时刻任何用户或节点都可以读取到最近一次成功提交的副本数据.需要配合一个获取最新成功提交的版本好的metadata服务,这样可以确定最心烦已成功提交的版本好,然后从已经读取的数据中就可以确认最新写入的数据 R = (A-W+1)

# 简述Paxos算法

Paxos是一个解决强一致性的思想、协议,并不是工程实现方法

 Paxos算法是解决的是一个分布式系统如何就某个值(决议)达成一致.一个经典的场景是,在一个分布式数据库系统中,如果各个节点的初始状态一致,每个节点执行相同的操作序列,那么他们最后能够得到一个一致的状态.为了保证每个节点执行相同的操作序列,需要在每一条指令上执行一个"一致性算法"以保证每个节点看到的指令一致.在parox算法中,有三种角色:Proposer(提议者),Acceptor(接受者),Learners(记录员)

+ Proposer提议者:只要Proposer发的提案Propose被板书以上的Acceptor接受,Proposer就认为该提案的value被选定了
+ Acceptor接受者:只要Acceptor接受的某个提案,Acceptor就认为改提案的value被选定
+ Learners记录员:Acceptor告诉Learner哪个value被选定,learner就认为哪个value被选定

Paxos算法 分为两个阶段,具体如下:

阶段一(prepare):

a. Proposer收到的client请求或者发现本地有未提交的值,选择一个提案编号N,然后向半数以上的Acceptor发送编号为N的Prepare请求.N为单调递增,且不会相等,如果请求发送的N与本地存储编号相同,这拒绝响应

b. Acceptor收到一个编号为N的Prepare请求,如果本轮Paxos

- 本节点已经有已提交的value记录,对比记录的编号和接受到的Prepare编号N,大于N则拒绝回应,否则返回该记录value及编号
- 没有已提交记录,判断本地是否有编号N1,N1>N则决绝响应,否则将N1改为N(如果没有N1,则记录N),并响应prepare

阶段二(accept)

a. 如果Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求,那么他就会发送一个针对[N,V]提案的Accept请求给半数以上Acceptor. V就是接收到的响应中编号最大的value,如果响应中不包含任何value, 那么V就由Proposer自己决定.

b. 如果Acceptor收到一个针对编号N的提案的Accept请求, Acceptor对比本地记录的编号,如果小于等于N, 则接受该值, 并提交记录value. 否则决绝请求.

Proposer如果收到大多数Acceptor响应, 则选定该value值,并同步给Learner, 使未响应的Acceptor达成一致.

活锁: accept是被拒绝,加大N, 重新accept, 此时另外一个proposer也在进行相同操作, 导致accept一直失败,无法完成算法

multi-paxos: 区别paxos只是确定一个值, multi-paxos可以确定多个值, 接收到accept请求后, 则一定时间内不再accept其他节点的请求,以保证后续的编号不需要经过prepare确认, 直接进行accept操作. 此时该节点成为了leader, 知道accept被拒绝, 重新发起prepare请求竞争leader资格

# 简述raft算法

raft在paxos基础上进行优化,可以应用在工程中.

概念:

+ 分布式一致性算法: raft会先选举出leader, leader完全负责replicated log的管理. leader负责接受所有客户端更新请求, 然后复制到follower节点, 并在"安全"的时候执行这些请求. 如果leader故障, follower会重新选举出新的leader
+ 三种状态: 一个节点任意时刻处在三者之一
  + leader: 处理所有的客户端请求(如果客户端将请求发送给了Follower,Follower将请求重新定向给Leader)
  + follower: 不会发送任何请求, 只会简单地响应来自Leader或Candidate的请求
  + candidate: 用于选举 产生新的leader
+ term:　任期, leader产生到重新选举为一任期, 每个节点都维持的当前任期号
  + term是递增的, 存储在log日志的entry中, 代表当前entry是在哪一个term时期写入
  + 每个任期只能有一个leader或者没有(选举是失败)
  + 每次RPC通信时传递该任期号, 如果RPC收到任期号大于本地的, 切换为follower, 小于本地任期号则返回错误信息
+ 两个RPC通信:
  + RequestVote RPC: 负责选举, 包含参数lastIndex(状态机中最后一条索引), lastTerm(任期号)
  + AppendEntries RPC: 负责数据的交互
+ 日志序列: 每一个节点上维持着一份持久化Log, 通过一致性协议算法, 保证每一个节点中的Log保持一致, 并且顺序存放, 这样客户端可以在每一个节点中读取到相同的数据
+ 状态机: 日志序列同步到多数节点时, leader将该日志提交到装态机, 并且在下一次心跳通知所有节点提交状态机(携带最后提交的lastIndex)

何时触发选举: 

- 集群初始化时, 都是follower, 随机超时, 最先结束超时的节点变成candidate, 发起选举
- 如果follower在election timeout内没有收到来自leader的心跳, 则主动触发选举

选举过程: 发出选举的节点角度

1. 增加节点本地的termID, 切换到candidate状态

2. 投自己一票

   其他节点投票逻辑: 每个节点同意任期最多投一票, 候选人知道的信息不能比自己小(通过副本日志lastIndex和安全机制保障), 先来先得

3. 并行给其他节点发送RequestVote RPCs(请求选举), 包含term 参数

4. 等待回复

   4.1 收到majority(大多数的)投票, 赢得选举, 切换到leader状态, 立刻给所有节点发信条信息

   4.2 被告之别人当选, 切换到follower状态.(原来的leader对比term, 比自己的大, 转换到follower状态)

   4.3 一段时间没收到majority和follower的心跳通知, 则保持candidate, 重新打出选举

日志序列同步:日志需要存储在磁盘持久化，　崩溃可以从日志恢复

1. 客户端发送命令给leader

2. leader把日志条目加到自己的日志序列里

3. Leader发送AppendEntries RPC请求给所有的follower. 携带了prevLogIndex, prevLogTerm 

   follower收到后, 进行日志序列匹配

   - 匹配上则追加到自己的日志序列
   - 匹配不上则拒绝请求, leader将日志index调小, 重新同步直至匹配上, follower将leader的日志序列覆盖到本地

一旦新的日志序列条目变成majority的了, 将日志序列应用到状态机中

+ Leader在状态机里提交自己日志序列条目, 然后返回结果给客户端
+ Leader下次发送AppendEntries RPC时, 告知follower已提交的日志序列条目信息(lastIndex)
+ follower收到RPC后, 提交到自己的状态机里

提交状态机时, 如果term为上一任期, 必须与当前任期数据一起提交, 否则可能出现覆盖已提交状态机的日志

新选举出的leader一定拥有自己所提交状态机的日志条目

- leader在当日志序列条目已经复制到大多数follower机器上时,才会提交日志条目
- 而选出的leader的logIndex必须大于等于大多数节点,因此leader肯定有最新的日志

安全原则:

+ 选举安全原则: 对于一个给定的任期号, 最多只会有一个领导人被选举出来
+ 状态机安全原则: 如果一个leader已经在给定的索引值位置的日志条目应用到状态机中, 那么其他任何的服务器在这个索引位置不会提交一个不同的日志
+ 领导人完全原则: 如果某个日志条目在某个任期号中已经被提交, 那么这个挑眉必然出现在更大的任期号的所有leader中
+ leader只能附件原则: leader绝对不会删除或者覆盖自己的日志, 只会增加
+ 日志匹配原则: 如果两个日志在相同索引位置的日志条目的任期号相同, 那么我们就认为这个日志从头到这个索引位置之间全部完全相同

# 简述zab协议

ZAB协议是为分布式协调服务Zookeeper专门设计的一种支持崩溃回复的院子广播协议,实现分布式数据一致性所有客户端的请求都是写入到Leader进程中, 然后, 由Leader同步到其他节点, 称为Follower. 在集群数据同步的过程中,如果出现follower节点崩溃或者Leader进程崩溃时, 都会通过Zab协议来保证数据一致性

ZAB协议包括两种基本的模式: **崩溃恢复和消息广播**

**消息广播:**

集群中所有事务(写请求)都由Leader节点来处理, 其他服务器为Follower, Leader, Leader将客户端的事务请求转换为事务Proposal, 并且将Proposal分发给集群中其他的所有的Follower

完成广播之后, Leader等待Follower反馈, 当所有过半数的Follower反馈信息后, Leader将再次向集群内Follower广播commit信息, Commit信息就是确认将之前的Proposal提交

Leader节点的写入是一个两步操作,第一步是广播事务操作, 第二步是广播提交操作, 其中过半数值得是反馈的节点书>=N/2+1, N是全部的Follower节点数量

**崩溃恢复**

时机:

+ 初始化集群, 刚刚启动的时候
+ Leader崩溃, 因为故障宕机
+ Leader失去了半数的机器支持, 与集群中超过一般的节点断连

此时开启新的一轮Leader选举,选举产生的Leader会与过半的Follower进行同步, 是数据一致, 当与过半的机器同步完成后, 就退出恢复模式, 然后进入消息广播模式

整个Zookeeper集群的一致性保证就是在上面两个状态之间切换, 当Leader服务正常时, 就是正常的消息广播模式, 当Leader不可用时, 则进入崩溃恢复模式, 崩溃恢复阶段会进行数据同步, 完成以后, 重新进入消息关闭阶段.

Zxid是ZAB协议的一个事务编号, Zxid是一个64位的数字, 其中低32位事宜个简单的单调递增计数器, 针对客户端每一个事务请求, 计数器加1; 而高32位则代表Leader周期年代的编号.

Leader周期(epoch), 可以理解为当前集群所处的年代或者周期, 每当有一个新的Leader选举出来时, 就会从这个Leader服务器上取出其本地日志中最大事务的Zxid, 并从中读取epoch值, 然后加1, 以此作为新的周期ID. 高32位代表了每代Leader的唯一性, 低32位就代表了每代Leader中事务的唯一性.

**zab节点的三种状态**: 每个节点任何时间都属于三种状态之一

following: 服从leader的命令

leader: 负责协调事务

election/looking: 选举状态

**Zab选举Leader流程**

# 负载均衡策略

1、轮询法

将请求按照顺序轮流地分配到后端服务器上， 它均衡地对待后端的每一台服务器， 而不关心服务器实际的连接数和前的系统负载

2、加权轮询法

不同的后端服务器可能机器的配置和当前系统的负载并不相同， 因此它们的抗压能力也不相同. 给配置高、负载低的机器配置更高的权重，让其处理更多的请求；而配置低、负载高的机器，给其分配较低的权重，降低其系统负载，加权轮询能很好地处理这一问题，并且请求顺序且按照权重分配到后端。

3、随机法

通过系统随机算法，根据后端服务的列表大小值来随机选取其中一台服务器进行访问。由概率统计理论可以得知，随着客户端调用服务端的次数增多，其实际效果越来越近乎平均分配调用量到后端的每一台服务器，也就是轮询的结果。

4、加权随机法

与加权轮询法一样，加权随机法也根据后端机器的配置，系统的负载分配不同的权重。不同的是，它是按照权重随机请求后端服务器，而非顺序。

5、原地址哈希法

源地址哈希的思想是根据客户端的IP地址（或其他标识号），通过哈希函数计算得到的一个数值，用该数值对服务器列表大小进行取模运算，得到的结果便是客户端要访问的服务期序号。采用源地址哈希法进行负载均衡，同一个IP地址的客户端，当后端服务器列表不变是，他每次都会映射到同一台后端服务器进行访问。但实际上是对集群进行割裂，每个服务器固定服务特定IP的客户请求，另外客户会发生IP飘移，会切换响应服务器，有session的问题

6、最小连接数法

最小连接数法比较灵活智能，由于后端服务器的配置不尽相同，对于请求处理有快有慢，他是根据后端服务器当前连接情况，动态地选取起哄当前积压连接数最少的一台服务器来处理当前的请求，尽可能地提高后端服务的利用效率，将负载合理地分流到每一台服务器

# 集群、分布式、SOA、微服务的概念及区别

SOA面向服务，处理分布式中各个服务模块交互，使用ESB总线完成服务间通信，系统庞大时候，因为ESB是中心结构，ESB就会成为系统瓶颈，

微服务提供去中心化的方式解决各模块间的通信，以注册发现机制实现，进行更细化的服务分类

集群：不同服务器部署同一套应用服务对外提供访问，实现服务的负载均衡或着互为备份（热备，主从等），指同一种组件的多个实例，形成的逻辑上的整体。单个节点可以提供完整服务。集群是物理形态。

分布式：服务的不同模块部署在不同的服务器上，单个节点不能提供完整的服务，需要多节点协调提供服务（也可以是相同组件部署在不同节点，但节点同过交换信息协作提供服务），分布式强调的是工作模式。

SOA：面向服务的架构，一种设计方法，其中包含多个服务，服务之间通过互相依赖最终提供一系列的功能。一个服务通常以独立的形式存在与操作系统进程中。各个服务之间通过网络调用。

- 中心化实现：ESB（企业服务总线），各服务通过ESB进行交互，解决易购系统之间的连通性，通过协议转换，消息解析，消息路由把服务提供者的数据传送到服务小分着。很重，有一定的逻辑，可以解决一些公用逻辑的问题
- 去中心化实现：微服务

微服务：在SOA上做的升华，微服务架构强调的一个重点是业务需要彻底的组件化和服务化，原有的单个业务系统会拆分为多个，可以独立开发，设计，运行的小应用。这些小应用之间通过服务完成交互和集成

服务单一职责

轻量级通信：去掉ESB总线，采用restAPI通信

